{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gensim \n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "stop_word = []\n",
    "with open(r\"C:\\Users\\ADMIN\\Documents\\vietnamese_stopwords.txt\",encoding=\"utf-8\") as f :\n",
    "    text = f.read()\n",
    "    for word in text.split() :\n",
    "        stop_word.append(word)\n",
    "    f.close()\n",
    "punc = list(punctuation)\n",
    "stop_word = stop_word + punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kiem toan', 'thanh nien', 'thanh tra', 'cong nghiep', 'quoc phong', 'bien dao', 'chinh sach', 'quoc hoi', 'kinh te', 'tai chinh', 'buu chinh vien thong', 'thoi trang', 'van hoa', 'xay dung', 'du lich', 'the thao', 'hai quan', 'phap luat', 'xa hoi', 'giao duc', 'giai tri', 'suc khoe', 'kinh doanh', 'bat dong san', 'giao thong van tai', 'y te', 'chinh tri', 'tai nguyen moi truong', 'phu nu', 'khoa hoc cong nghe', 'thuong mai', 'van nghe', 'do thi']\n"
     ]
    }
   ],
   "source": [
    "label = ['an ninh', 'an ninh trat tu', 'bat dong san', 'bien dao',\n",
    "       'buu chinh vien thong', 'chinh sach', 'chinh tri', 'cong nghiep',\n",
    "       'dan toc', 'dau thau', 'do thi', 'doi ngoai', 'du lich',\n",
    "       'giai tri', 'giao duc', 'giao thong van tai', 'hai quan',\n",
    "       'khoa hoc cong nghe', 'kiem toan', 'kinh doanh', 'kinh te',\n",
    "       'lao dong', 'lao dong thuong binh va xa hoi', 'nang luong',\n",
    "       'nong nghiep', 'phap luat', 'phu nu', 'quoc hoi', 'quoc phong',\n",
    "       'so huu tri tue', 'suc khoe', 'tai chinh', 'tai nguyen moi truong',\n",
    "       'thanh nien', 'thanh tra', 'the thao', 'thoi trang',\n",
    "       'thong tin va truyen thong', 'thuong mai', 'tre em', 'van hoa',\n",
    "       'van nghe', 'xa hoi', 'xay dung', 'y te']\n",
    "tmp=[]\n",
    "\n",
    "for i in ([18, 33, 34, 7, 28, 3, 5, 27, 20, 31, 4, 36, 40, 43, 12, 35, 16, 25, 42, 14, 13, 30, 19, 2, 15, 44, 6, 32, 26, 17, 38, 41, 10]):\n",
    "    tmp.append(label[i])\n",
    "print(tmp)\n",
    "\n",
    "ls = ['kiem toan', 'thanh nien', 'thanh tra', 'cong nghiep', 'quoc phong', 'bien dao', \n",
    "      'chinh sach', 'quoc hoi', 'kinh te', 'tai chinh', 'buu chinh vien thong',\n",
    "      'thoi trang', 'van hoa', 'xay dung', 'du lich', 'the thao', 'hai quan', 'phap luat', \n",
    "      'xa hoi', 'giao duc', 'giai tri', 'suc khoe', 'kinh doanh', 'bat dong san',\n",
    "      'giao thong van tai', 'y te', 'chinh tri', 'tai nguyen moi truong', 'phu nu',\n",
    "      'khoa hoc cong nghe', 'thuong mai', 'van nghe', 'do thi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an_ninh_05.json\n",
      "an_ninh_06.json\n",
      "an_ninh_07.json\n",
      "an_ninh_trat_tu.json\n",
      "bat_dong_san.json\n",
      "bien_dao.json\n",
      "bien_dao_08_2.json\n",
      "buu_chinh_vien_thong.json\n",
      "chinh_sach.json\n",
      "chinh_sach_08_2.json\n",
      "chinh_tri.json\n",
      "cong_nghiep_04.json\n",
      "cong_nghiep_05.json\n",
      "cong_nghiep_06.json\n",
      "cong_nghiep_07.json\n",
      "cong_nghiep_08_2.json\n",
      "dan_toc_04.json\n",
      "dan_toc_05.json\n",
      "dan_toc_06.json\n",
      "dan_toc_07.json\n",
      "dau_thau_03.json\n",
      "dau_thau_04.json\n",
      "dau_thau_05.json\n",
      "dau_thau_06.json\n",
      "doi_ngoai_04.json\n",
      "doi_ngoai_05.json\n",
      "doi_ngoai_06.json\n",
      "doi_ngoai_07.json\n",
      "do_thi.json\n",
      "du_lich.json\n",
      "giai_tri.json\n",
      "giao_duc.json\n",
      "giao_thong_van_tai_02.json\n",
      "giao_thong_van_tai_03.json\n",
      "giao_thong_van_tai_04.json\n",
      "giao_thong_van_tai_05.json\n",
      "giao_thong_van_tai_06.json\n",
      "giao_thong_van_tai_07.json\n",
      "giao_thong_van_tai_08.json\n",
      "hai_quan.json\n",
      "khoa_hoc_cong_nghe_04.json\n",
      "khoa_hoc_cong_nghe_05.json\n",
      "khoa_hoc_cong_nghe_06.json\n",
      "khoa_hoc_cong_nghe_07.json\n",
      "khoa_hoc_cong_nghe_2_02.json\n",
      "khoa_hoc_cong_nghe_2_03.json\n",
      "khoa_hoc_cong_nghe_2_04.json\n",
      "khoa_hoc_cong_nghe_2_05.json\n",
      "khoa_hoc_cong_nghe_2_06.json\n",
      "khoa_hoc_cong_nghe_2_07.json\n",
      "khoa_hoc_cong_nghe_2_08.json\n",
      "kiem_toan_02.json\n",
      "kiem_toan_03.json\n",
      "kiem_toan_04.json\n",
      "kiem_toan_05.json\n",
      "kiem_toan_06.json\n",
      "kiem_toan_06_2.json\n",
      "kiem_toan_07.json\n",
      "kiem_toan_08.json\n",
      "kinh_doanh.json\n",
      "kinh_te.json\n",
      "lao_dong_02.json\n",
      "lao_dong_03.json\n",
      "lao_dong_04.json\n",
      "lao_dong_05.json\n",
      "lao_dong_06.json\n",
      "lao_dong_07.json\n",
      "lao_dong_thuong_binh_va_xa_hoi_02.json\n",
      "lao_dong_thuong_binh_va_xa_hoi_02_02.json\n",
      "lao_dong_thuong_binh_va_xa_hoi_02_03.json\n",
      "lao_dong_thuong_binh_va_xa_hoi_02_05.json\n",
      "lao_dong_thuong_binh_va_xa_hoi_02_06.json\n",
      "lao_dong_thuong_binh_va_xa_hoi_02_07.json\n",
      "lao_dong_thuong_binh_va_xa_hoi_03.json\n",
      "lao_dong_thuong_binh_va_xa_hoi_04.json\n",
      "lao_dong_thuong_binh_va_xa_hoi_05.json\n",
      "lao_dong_thuong_binh_va_xa_hoi_06.json\n",
      "lao_dong_thuong_binh_va_xa_hoi_07.json\n",
      "nang_luong_01_02.json\n",
      "nang_luong_01_03.json\n",
      "nang_luong_01_04.json\n",
      "nang_luong_01_05.json\n",
      "nang_luong_01_06.json\n",
      "nang_luong_01_07.json\n",
      "nang_luong_02_02.json\n",
      "nang_luong_02_03.json\n",
      "nang_luong_02_04.json\n",
      "nang_luong_02_05.json\n",
      "nang_luong_02_06.json\n",
      "nang_luong_02_07.json\n",
      "nong_nghiep_06.json\n",
      "nong_nghiep_07.json\n",
      "phap_luat.json\n",
      "phu_nu.json\n",
      "quoc_hoi_02.json\n",
      "quoc_hoi_03.json\n",
      "quoc_hoi_04.json\n",
      "quoc_hoi_05.json\n",
      "quoc_hoi_06.json\n",
      "quoc_hoi_08_2.json\n",
      "quoc_phong.json\n",
      "quoc_phong_08_2.json\n",
      "so_huu_tri_tue_06.json\n",
      "so_huu_tri_tue_07.json\n",
      "suc_khoe.json\n",
      "tai_chinh.json\n",
      "tai_nguyen_moi_truong_02.json\n",
      "tai_nguyen_moi_truong_03.json\n",
      "tai_nguyen_moi_truong_04.json\n",
      "tai_nguyen_moi_truong_05.json\n",
      "tai_nguyen_moi_truong_06.json\n",
      "tai_nguyen_moi_truong_07.json\n",
      "thanh_nien_04.json\n",
      "thanh_nien_05.json\n",
      "thanh_nien_06.json\n",
      "thanh_nien_07.json\n",
      "thanh_tra_02.json\n",
      "thanh_tra_03.json\n",
      "thanh_tra_04.json\n",
      "thanh_tra_05.json\n",
      "thanh_tra_06.json\n",
      "thanh_tra_07.json\n",
      "thanh_tra_08.json\n",
      "the_thao.json\n",
      "thoi_trang.json\n",
      "thong_tin_va_truyen_thong_02.json\n",
      "thong_tin_va_truyen_thong_03.json\n",
      "thong_tin_va_truyen_thong_04.json\n",
      "thong_tin_va_truyen_thong_05.json\n",
      "thong_tin_va_truyen_thong_06.json\n",
      "thong_tin_va_truyen_thong_07.json\n",
      "thuong_mai.json\n",
      "tre_em_06.json\n",
      "tre_em_07.json\n",
      "van_hoa.json\n",
      "van_nghe_02.json\n",
      "van_nghe_03.json\n",
      "van_nghe_04.json\n",
      "van_nghe_05.json\n",
      "van_nghe_06.json\n",
      "van_nghe_07.json\n",
      "xay_dung.json\n",
      "xa_hoi.json\n",
      "y_te.json\n"
     ]
    }
   ],
   "source": [
    "X=[]\n",
    "y=[]\n",
    "\n",
    "folder_path = r\"C:\\Users\\ADMIN\\Desktop\\json1\\json1\"\n",
    "def get_label(s):   \n",
    "    s = s.replace('.json','')\n",
    "    \n",
    "    for i in range(10):\n",
    "        s = s.replace(str(i),'')\n",
    "    \n",
    "    s = s.split('_')\n",
    "    s = ' '.join(s)\n",
    "    s = s.replace('  ','')\n",
    "    if s[len(s)-1] == ' ':\n",
    "        s = s[:len(s)-1]\n",
    "    return s\n",
    "\n",
    "X=[]\n",
    "y=[]\n",
    "\n",
    "folder_path = r\"C:\\Users\\ADMIN\\Desktop\\json1\\json1\"\n",
    "def get_label(s):   \n",
    "    s = s.replace('.json','')\n",
    "    \n",
    "    for i in range(10):\n",
    "        s = s.replace(str(i),'')\n",
    "    \n",
    "    s = s.split('_')\n",
    "    s = ' '.join(s)\n",
    "    s = s.replace('  ','')\n",
    "    if s[len(s)-1] == ' ':\n",
    "        s = s[:len(s)-1]\n",
    "    return s\n",
    "\n",
    "dirs = os.listdir(folder_path)\n",
    "for path in (dirs):\n",
    "    with open (os.path.join(folder_path, path), encoding = \"utf8\",) as f:\n",
    "        data = json.load(f)\n",
    "        print(path)\n",
    "        for i in data[\"response\"][\"docs\"]:\n",
    "            k = i[\"message\"]\n",
    "            if k!= '':\n",
    "                k = get_label(path)\n",
    "                if k in ls:\n",
    "                    k = i[\"message\"]\n",
    "                    k = gensim.utils.simple_preprocess(k)\n",
    "                    tmp = []\n",
    " \n",
    "                    for i in k:\n",
    "                        if i not in stop_word:\n",
    "                            tmp.append(i)\n",
    "                    k = tmp \n",
    "                    k = ' '.join(k)\n",
    "                    k = ViTokenizer.tokenize(k)\n",
    "                    X.append(k)\n",
    "                    k = get_label(path)\n",
    "                    y.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 131555)\t0.012218982567492794\n",
      "  (0, 130948)\t0.0639085013480694\n",
      "  (0, 130638)\t0.03265579852500959\n",
      "  (0, 130086)\t0.030660967112460455\n",
      "  (0, 130069)\t0.03576458987648852\n",
      "  (0, 129830)\t0.12264193758623168\n",
      "  (0, 129690)\t0.017283929444869298\n",
      "  (0, 129511)\t0.09346667882419557\n",
      "  (0, 129141)\t0.035767547311168206\n",
      "  (0, 129022)\t0.3244871801945953\n",
      "  (0, 128628)\t0.02136292516403564\n",
      "  (0, 128172)\t0.025128702091482644\n",
      "  (0, 126774)\t0.039791617812255604\n",
      "  (0, 125278)\t0.024893382495047175\n",
      "  (0, 122573)\t0.06831640142366215\n",
      "  (0, 121884)\t0.052986613754356274\n",
      "  (0, 120976)\t0.032561989377081255\n",
      "  (0, 119727)\t0.03093461556611553\n",
      "  (0, 119220)\t0.03296044112625355\n",
      "  (0, 119026)\t0.023744760291913514\n",
      "  (0, 118976)\t0.01477856651762714\n",
      "  (0, 118850)\t0.044978872616956454\n",
      "  (0, 117753)\t0.025652132043447882\n",
      "  (0, 116192)\t0.02974594518337779\n",
      "  (0, 113268)\t0.05455829240304309\n",
      "  :\t:\n",
      "  (62922, 36458)\t0.07210006811088626\n",
      "  (62922, 34984)\t0.059180066526609156\n",
      "  (62922, 33970)\t0.04495046277620586\n",
      "  (62922, 31765)\t0.04365302998944723\n",
      "  (62922, 30642)\t0.017118525300626613\n",
      "  (62922, 25927)\t0.029994221252354587\n",
      "  (62922, 25652)\t0.03493253730762521\n",
      "  (62922, 25099)\t0.055148386451647255\n",
      "  (62922, 24477)\t0.04266771864509092\n",
      "  (62922, 23921)\t0.034294411962107044\n",
      "  (62922, 23888)\t0.04268661123974586\n",
      "  (62922, 23245)\t0.015281178641145596\n",
      "  (62922, 19692)\t0.051207011102470154\n",
      "  (62922, 19413)\t0.02504372738602071\n",
      "  (62922, 18696)\t0.030750250072225288\n",
      "  (62922, 14462)\t0.01524147048513086\n",
      "  (62922, 14242)\t0.03405608221735049\n",
      "  (62922, 10931)\t0.03207872512708916\n",
      "  (62922, 10667)\t0.056127784862211136\n",
      "  (62922, 10665)\t0.6721777400187249\n",
      "  (62922, 10323)\t0.019473145459343304\n",
      "  (62922, 7749)\t0.031456837402675415\n",
      "  (62922, 7587)\t0.04578284290974508\n",
      "  (62922, 5446)\t0.027823071516702765\n",
      "  (62922, 1734)\t0.012045743499882553\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word')\n",
    "tfidf_vect.fit(X)\n",
    "X_tfidf =  tfidf_vect.transform(X)\n",
    "print(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bat dong san', 'bien dao', 'buu chinh vien thong', 'chinh sach',\n",
       "       'chinh tri', 'cong nghiep', 'do thi', 'du lich', 'giai tri',\n",
       "       'giao duc', 'giao thong van tai', 'hai quan', 'khoa hoc cong nghe',\n",
       "       'kiem toan', 'kinh doanh', 'kinh te', 'phap luat', 'phu nu',\n",
       "       'quoc hoi', 'quoc phong', 'suc khoe', 'tai chinh',\n",
       "       'tai nguyen moi truong', 'thanh nien', 'thanh tra', 'the thao',\n",
       "       'thoi trang', 'thuong mai', 'van hoa', 'van nghe', 'xa hoi',\n",
       "       'xay dung', 'y te'], dtype='<U21')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "encoder = sklearn.preprocessing.LabelEncoder()\n",
    "y = encoder.fit_transform(y)\n",
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "encoder = sklearn.preprocessing.LabelEncoder()\n",
    "y = encoder.fit_transform(y)\n",
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 128742)\t0.041697383640503016\n",
      "  (0, 127688)\t0.09112640490155519\n",
      "  (0, 126931)\t0.033727732696700784\n",
      "  (0, 126926)\t0.026032616295909507\n",
      "  (0, 126856)\t0.04092439492409045\n",
      "  (0, 126855)\t0.031589883085735286\n",
      "  (0, 125741)\t0.08775046810625435\n",
      "  (0, 125730)\t0.04513701300701569\n",
      "  (0, 125644)\t0.03510397859692355\n",
      "  (0, 124264)\t0.04224751757876029\n",
      "  (0, 124262)\t0.032716021085342974\n",
      "  (0, 124255)\t0.03959027174676296\n",
      "  (0, 124253)\t0.04025309560069598\n",
      "  (0, 124252)\t0.0651131129369971\n",
      "  (0, 124251)\t0.04356536267766465\n",
      "  (0, 124043)\t0.03591107029429077\n",
      "  (0, 124019)\t0.041802775256897996\n",
      "  (0, 124018)\t0.04120074937745406\n",
      "  (0, 123564)\t0.04110696119919733\n",
      "  (0, 123252)\t0.04202031073776316\n",
      "  (0, 123168)\t0.03381624248071934\n",
      "  (0, 123165)\t0.245013001773395\n",
      "  (0, 123048)\t0.04420511059034946\n",
      "  (0, 122435)\t0.041697383640503016\n",
      "  (0, 122421)\t0.021888780577157117\n",
      "  :\t:\n",
      "  (62922, 33405)\t0.06826905772709664\n",
      "  (62922, 33335)\t0.06290524047648592\n",
      "  (62922, 32052)\t0.06973953247094025\n",
      "  (62922, 32045)\t0.09999896610056311\n",
      "  (62922, 31955)\t0.06798837255638153\n",
      "  (62922, 31908)\t0.07757764794424142\n",
      "  (62922, 31397)\t0.0830856967959832\n",
      "  (62922, 30142)\t0.08259021736027346\n",
      "  (62922, 28443)\t0.08235147029273873\n",
      "  (62922, 25853)\t0.07108138740080616\n",
      "  (62922, 23286)\t0.07596748777053343\n",
      "  (62922, 22139)\t0.13100924137989833\n",
      "  (62922, 19578)\t0.07308635803400165\n",
      "  (62922, 17354)\t0.07687469192631134\n",
      "  (62922, 17240)\t0.05500063573763456\n",
      "  (62922, 11158)\t0.059055163799538186\n",
      "  (62922, 10553)\t0.06973953247094025\n",
      "  (62922, 8180)\t0.4284253850472322\n",
      "  (62922, 8179)\t0.08970223224743311\n",
      "  (62922, 3972)\t0.08749143218582658\n",
      "  (62922, 3846)\t0.05035712801914213\n",
      "  (62922, 2104)\t0.08235147029273873\n",
      "  (62922, 2098)\t0.05404821248622109\n",
      "  (62922, 822)\t0.06798837255638153\n",
      "  (62922, 817)\t0.04006585067410027\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', max_features=130000, ngram_range=(2, 3))\n",
    "tfidf_vect_ngram.fit(X)\n",
    "X_tfidf_ngram =  tfidf_vect_ngram.transform(X)\n",
    "print(X_tfidf_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1734)\t3\n",
      "  (0, 4426)\t1\n",
      "  (0, 5710)\t2\n",
      "  (0, 5747)\t1\n",
      "  (0, 7481)\t2\n",
      "  (0, 8159)\t1\n",
      "  (0, 8271)\t1\n",
      "  (0, 9130)\t12\n",
      "  (0, 9254)\t1\n",
      "  (0, 10802)\t1\n",
      "  (0, 11093)\t4\n",
      "  (0, 13211)\t1\n",
      "  (0, 13509)\t1\n",
      "  (0, 13647)\t1\n",
      "  (0, 14578)\t1\n",
      "  (0, 14975)\t1\n",
      "  (0, 15124)\t1\n",
      "  (0, 16267)\t2\n",
      "  (0, 17582)\t2\n",
      "  (0, 18605)\t2\n",
      "  (0, 19702)\t1\n",
      "  (0, 20731)\t2\n",
      "  (0, 22873)\t2\n",
      "  (0, 23937)\t1\n",
      "  (0, 24481)\t2\n",
      "  :\t:\n",
      "  (62922, 104670)\t2\n",
      "  (62922, 105827)\t3\n",
      "  (62922, 105948)\t1\n",
      "  (62922, 106049)\t1\n",
      "  (62922, 106415)\t2\n",
      "  (62922, 107209)\t1\n",
      "  (62922, 107956)\t3\n",
      "  (62922, 108333)\t1\n",
      "  (62922, 108496)\t1\n",
      "  (62922, 108583)\t1\n",
      "  (62922, 110435)\t3\n",
      "  (62922, 111476)\t1\n",
      "  (62922, 111864)\t5\n",
      "  (62922, 113310)\t1\n",
      "  (62922, 114319)\t3\n",
      "  (62922, 117418)\t1\n",
      "  (62922, 122863)\t1\n",
      "  (62922, 123454)\t7\n",
      "  (62922, 127912)\t1\n",
      "  (62922, 128465)\t4\n",
      "  (62922, 129690)\t3\n",
      "  (62922, 129940)\t1\n",
      "  (62922, 131008)\t1\n",
      "  (62922, 131863)\t1\n",
      "  (62922, 131922)\t5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "count_vect = CountVectorizer(analyzer ='word', token_pattern =r'\\w{1,}')\n",
    "count_vect.fit(X)\n",
    "X_count = count_vect.transform(X)\n",
    "print(X_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 128742)\t0.041697383640503016\n",
      "  (0, 127688)\t0.09112640490155519\n",
      "  (0, 126931)\t0.033727732696700784\n",
      "  (0, 126926)\t0.026032616295909507\n",
      "  (0, 126856)\t0.04092439492409045\n",
      "  (0, 126855)\t0.031589883085735286\n",
      "  (0, 125741)\t0.08775046810625435\n",
      "  (0, 125730)\t0.04513701300701569\n",
      "  (0, 125644)\t0.03510397859692355\n",
      "  (0, 124264)\t0.04224751757876029\n",
      "  (0, 124262)\t0.032716021085342974\n",
      "  (0, 124255)\t0.03959027174676296\n",
      "  (0, 124253)\t0.04025309560069598\n",
      "  (0, 124252)\t0.0651131129369971\n",
      "  (0, 124251)\t0.04356536267766465\n",
      "  (0, 124043)\t0.03591107029429077\n",
      "  (0, 124019)\t0.041802775256897996\n",
      "  (0, 124018)\t0.04120074937745406\n",
      "  (0, 123564)\t0.04110696119919733\n",
      "  (0, 123252)\t0.04202031073776316\n",
      "  (0, 123168)\t0.03381624248071934\n",
      "  (0, 123165)\t0.245013001773395\n",
      "  (0, 123048)\t0.04420511059034946\n",
      "  (0, 122435)\t0.041697383640503016\n",
      "  (0, 122421)\t0.021888780577157117\n",
      "  :\t:\n",
      "  (62922, 33405)\t0.06826905772709664\n",
      "  (62922, 33335)\t0.06290524047648592\n",
      "  (62922, 32052)\t0.06973953247094025\n",
      "  (62922, 32045)\t0.09999896610056311\n",
      "  (62922, 31955)\t0.06798837255638153\n",
      "  (62922, 31908)\t0.07757764794424142\n",
      "  (62922, 31397)\t0.0830856967959832\n",
      "  (62922, 30142)\t0.08259021736027346\n",
      "  (62922, 28443)\t0.08235147029273873\n",
      "  (62922, 25853)\t0.07108138740080616\n",
      "  (62922, 23286)\t0.07596748777053343\n",
      "  (62922, 22139)\t0.13100924137989833\n",
      "  (62922, 19578)\t0.07308635803400165\n",
      "  (62922, 17354)\t0.07687469192631134\n",
      "  (62922, 17240)\t0.05500063573763456\n",
      "  (62922, 11158)\t0.059055163799538186\n",
      "  (62922, 10553)\t0.06973953247094025\n",
      "  (62922, 8180)\t0.4284253850472322\n",
      "  (62922, 8179)\t0.08970223224743311\n",
      "  (62922, 3972)\t0.08749143218582658\n",
      "  (62922, 3846)\t0.05035712801914213\n",
      "  (62922, 2104)\t0.08235147029273873\n",
      "  (62922, 2098)\t0.05404821248622109\n",
      "  (62922, 822)\t0.06798837255638153\n",
      "  (62922, 817)\t0.04006585067410027\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd_ngram = TruncatedSVD(n_components=300, random_state=42)\n",
    "svd_ngram.fit(X_tfidf_ngram)\n",
    "X_tfidf_ngram_svd = svd_ngram.transform(X_tfidf_ngram)\n",
    "print(X_tfidf_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.5993052717613404\n",
      "Validation accuracy:  0.5539015733432219\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split    \n",
    "    \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tfidf, y, test_size=0.3, random_state=42)\n",
    "       \n",
    "cl=MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "train_predictions = cl.predict(X_train)\n",
    "val_predictions = cl.predict(X_val)\n",
    "\n",
    "print(\"Train accuracy: \", sklearn.metrics.accuracy_score(train_predictions, y_train))\n",
    "print(\"Validation accuracy: \", sklearn.metrics.accuracy_score(val_predictions, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.7171366298869364\n",
      "Validation accuracy:  0.6362769507866716\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split    \n",
    "    \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tfidf_ngram, y, test_size=0.3, random_state=42)\n",
    "       \n",
    "cl=MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "train_predictions = cl.predict(X_train)\n",
    "val_predictions = cl.predict(X_val)\n",
    "\n",
    "print(\"Train accuracy: \", sklearn.metrics.accuracy_score(train_predictions, y_train))\n",
    "print(\"Validation accuracy: \", sklearn.metrics.accuracy_score(val_predictions, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\ADMIN\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.7514643781501158\n",
      "Validation accuracy:  0.6727763945542194\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tfidf, y, test_size=0.3, random_state=42)\n",
    "       \n",
    "cl=LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "train_predictions = cl.predict(X_train)\n",
    "val_predictions = cl.predict(X_val)\n",
    "\n",
    "print(\"Train accuracy: \", sklearn.metrics.accuracy_score(train_predictions, y_train))\n",
    "print(\"Validation accuracy: \", sklearn.metrics.accuracy_score(val_predictions, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6af42dd0bd1c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tfidf_ngram\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tfidf_ngram, y, test_size=0.3, random_state=42)\n",
    "       \n",
    "cl=LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "train_predictions = cl.predict(X_train)\n",
    "val_predictions = cl.predict(X_val)\n",
    "\n",
    "print(\"Train accuracy: \", sklearn.metrics.accuracy_score(train_predictions, y_train))\n",
    "print(\"Validation accuracy: \", sklearn.metrics.accuracy_score(val_predictions, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
